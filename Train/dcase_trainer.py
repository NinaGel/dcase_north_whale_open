#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DCASEè®­ç»ƒå™¨ / DCASE Trainer
ç”¨äºDCASE2020æ•°æ®é›†çš„å¸§çº§å£°éŸ³äº‹ä»¶æ£€æµ‹
Frame-level sound event detection for DCASE2020 dataset

ç‰¹ç‚¹ / Features:
- å¸§çº§æ ‡ç­¾è®­ç»ƒ / Frame-level label training
- 10ç±»äº‹ä»¶æ£€æµ‹ / 10-class event detection
- DCASEä¸“ç”¨è¯„ä¼°æŒ‡æ ‡ / DCASE-specific evaluation metrics
"""

import torch
import torch.nn as nn
import numpy as np
from torch.cuda import amp
from torch.nn.utils import clip_grad_norm_
from tqdm import tqdm
import time
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union, List

# å¯¼å…¥åŸºç¡€è®­ç»ƒå™¨
from Train.train_utils import BaseTrainer, MetricTracker
import config_dcase as cfg


class DCASEMetricTracker(MetricTracker):
    """DCASEæŒ‡æ ‡è¿½è¸ªå™¨ / DCASE Metric Tracker

    è¿½è¸ªå¸§çº§ã€äº‹ä»¶çº§ã€æ®µçº§F1ç­‰DCASEç‰¹æœ‰æŒ‡æ ‡
    Tracks frame-level, event-level, segment-level F1 and other DCASE metrics
    """

    def __init__(self):
        super().__init__()
        # DCASEç‰¹æœ‰æŒ‡æ ‡
        self.add_metric('frame_accuracy')     # å¸§çº§å‡†ç¡®ç‡
        self.add_metric('event_f1')          # äº‹ä»¶çº§F1
        self.add_metric('segment_f1')        # æ®µçº§F1
        self.add_metric('class_wise_f1')     # æ¯ç±»F1åˆ†æ•°
        
    def compute_frame_metrics(self, predictions, targets, threshold=0.5):
        """è®¡ç®—å¸§çº§æŒ‡æ ‡
        
        Args:
            predictions: [batch_size, frames, num_classes]
            targets: [batch_size, frames, num_classes]
            threshold: é˜ˆå€¼
        """
        # è½¬æ¢ä¸ºäºŒè¿›åˆ¶é¢„æµ‹
        pred_binary = (predictions > threshold).float()
        
        # å¸§çº§å‡†ç¡®ç‡ (è€ƒè™‘å¤šæ ‡ç­¾çš„æƒ…å†µ)
        correct_frames = (pred_binary == targets).all(dim=-1)  # [batch_size, frames]
        frame_acc = correct_frames.float().mean().item()
        
        self.update('frame_accuracy', frame_acc)
        
        return {
            'frame_accuracy': frame_acc,
            'pred_binary': pred_binary
        }
    
    def compute_class_wise_metrics(self, predictions, targets, threshold=0.5):
        """è®¡ç®—æ¯ç±»åˆ«çš„æŒ‡æ ‡"""
        pred_binary = (predictions > threshold).float()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°
        class_f1_scores = []
        num_classes = predictions.shape[-1]
        
        for class_idx in range(num_classes):
            pred_class = pred_binary[..., class_idx].flatten()
            target_class = targets[..., class_idx].flatten()
            
            # è®¡ç®—TP, FP, FN
            tp = ((pred_class == 1) & (target_class == 1)).sum().float()
            fp = ((pred_class == 1) & (target_class == 0)).sum().float()
            fn = ((pred_class == 0) & (target_class == 1)).sum().float()
            
            # è®¡ç®—F1åˆ†æ•°
            precision = tp / (tp + fp + 1e-8)
            recall = tp / (tp + fn + 1e-8)
            f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
            
            class_f1_scores.append(f1.item())
        
        avg_class_f1 = np.mean(class_f1_scores)
        self.update('class_wise_f1', avg_class_f1)
        
        return {
            'class_f1_scores': class_f1_scores,
            'avg_class_f1': avg_class_f1
        }


class DCASETrainer(BaseTrainer):
    """DCASEä¸“ç”¨è®­ç»ƒå™¨"""
    
    def __init__(self, model, optimizer, loss_fn, device, scaler=None):
        """åˆå§‹åŒ–DCASEè®­ç»ƒå™¨
        
        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨  
            loss_fn: æŸå¤±å‡½æ•°
            device: è®¾å¤‡
            scaler: æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨
        """
        # ä¸éœ€è¦SNRå¹³è¡¡å™¨ï¼Œå› ä¸ºDCASEæ•°æ®é›†æ²¡æœ‰SNRåˆ†ç»„
        super().__init__(model, optimizer, loss_fn, device, snr_balancer=None, scaler=scaler)
        
        # æ›¿æ¢ä¸ºDCASEä¸“ç”¨æŒ‡æ ‡è¿½è¸ªå™¨
        self.metric_tracker = DCASEMetricTracker()
        
        # DCASEç‰¹å®šé…ç½®
        self.num_classes = cfg.DCASE_MODEL_CONFIG['num_classes']
        self.class_names = cfg.DCASE_MODEL_CONFIG['class_names']
        
        logging.info(f"DCASEè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œç±»åˆ«æ•°: {self.num_classes}")
        
    def _initialize_scheduler(self):
        """åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨DCASEé…ç½®"""
        scheduler_config = cfg.DCASE_TRAIN_CONFIG.get('scheduler', {})
        scheduler_type = scheduler_config.get('type', 'cosine')
        
        if scheduler_type == 'cosine':
            from torch.optim.lr_scheduler import CosineAnnealingLR
            return CosineAnnealingLR(
                self.optimizer,
                T_max=scheduler_config.get('T_max', cfg.DCASE_TRAIN_CONFIG['epochs']),
                eta_min=scheduler_config.get('eta_min', 5e-7)
            )
        else:
            return None
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch - é€‚é…DCASEæ•°æ®æ ¼å¼"""
        self.model.train()
        total_loss = 0
        batch_times = []
        
        # è·å–æ¢¯åº¦è£å‰ªé…ç½®
        grad_clip = cfg.DCASE_TRAIN_CONFIG.get('grad_clip', False)
        grad_clip_value = cfg.DCASE_TRAIN_CONFIG.get('grad_clip_value', 1.0)
        
        current_lr = self.optimizer.param_groups[0]['lr']
        
        with tqdm(train_loader, desc=f"DCASE Epoch {self.current_epoch + 1} (lr={current_lr:.2e})") as pbar:
            for i, batch in enumerate(pbar):
                batch_start = time.time()
                
                # DCASEæ•°æ®æ ¼å¼: (features, labels, filenames)
                if len(batch) >= 3:
                    features, labels, filenames = batch[:3]
                elif len(batch) == 2:
                    features, labels = batch[:2]
                else:
                    raise ValueError(f"DCASE batchæ ¼å¼é”™è¯¯: {len(batch)}")
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                features = features.to(self.device, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                
                # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡® [batch_size, channel, n_mels, frames]
                if features.dim() == 3:  # [batch_size, n_mels, frames]
                    features = features.unsqueeze(1)  # [batch_size, 1, n_mels, frames]
                
                # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
                with amp.autocast(enabled=self.scaler.is_enabled()):
                    predictions = self.model(features)  # [batch_size, frames, num_classes]
                    
                    # ç¡®ä¿é¢„æµ‹å’Œæ ‡ç­¾ç»´åº¦åŒ¹é…
                    if predictions.shape != labels.shape:
                        logging.warning(f"ç»´åº¦ä¸åŒ¹é…: pred {predictions.shape}, label {labels.shape}")
                        # å¯èƒ½éœ€è¦è½¬ç½®æˆ–è°ƒæ•´ç»´åº¦
                        if len(predictions.shape) == 3 and len(labels.shape) == 3:
                            if predictions.shape[1] != labels.shape[1]:
                                predictions = predictions.transpose(1, 2)
                    
                    loss = self.loss_fn(predictions, labels)
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                if grad_clip:
                    self.scaler.unscale_(self.optimizer)
                    grad_norm = clip_grad_norm_(self.model.parameters(), grad_clip_value)
                    self.metric_tracker.update('grad_norms', grad_norm.item())
                
                # æ›´æ–°å‚æ•°
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.optimizer.zero_grad(set_to_none=True)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                batch_time = time.time() - batch_start
                batch_times.append(batch_time)
                
                # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
                with torch.no_grad():
                    frame_metrics = self.metric_tracker.compute_frame_metrics(
                        torch.sigmoid(predictions), labels
                    )
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'frame_acc': f'{frame_metrics["frame_accuracy"]:.3f}',
                    'time': f'{batch_time:.2f}s'
                })
        
        avg_loss = total_loss / len(train_loader)
        avg_time = np.mean(batch_times)
        
        return avg_loss, avg_time
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹ - é€‚é…DCASEæ•°æ®æ ¼å¼"""
        self.model.eval()
        total_loss = 0
        batch_count = 0
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            with amp.autocast(enabled=self.scaler.is_enabled()):
                for batch in tqdm(val_loader, desc="Validating", leave=False):
                    # DCASEæ•°æ®æ ¼å¼
                    if len(batch) >= 3:
                        features, labels, filenames = batch[:3]
                    elif len(batch) == 2:
                        features, labels = batch[:2]
                    else:
                        raise ValueError(f"DCASEéªŒè¯batchæ ¼å¼é”™è¯¯: {len(batch)}")
                    
                    features = features.to(self.device, non_blocking=True)
                    labels = labels.to(self.device, non_blocking=True)
                    
                    # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
                    if features.dim() == 3:
                        features = features.unsqueeze(1)
                    
                    predictions = self.model(features)
                    
                    # è°ƒæ•´ç»´åº¦åŒ¹é…
                    if predictions.shape != labels.shape:
                        if len(predictions.shape) == 3 and len(labels.shape) == 3:
                            if predictions.shape[1] != labels.shape[1]:
                                predictions = predictions.transpose(1, 2)
                    
                    loss = self.loss_fn(predictions, labels)
                    
                    total_loss += loss.item()
                    batch_count += 1
                    
                    # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾ç”¨äºæŒ‡æ ‡è®¡ç®—
                    all_predictions.append(torch.sigmoid(predictions).cpu())
                    all_targets.append(labels.cpu())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        if all_predictions:
            all_predictions = torch.cat(all_predictions, dim=0)
            all_targets = torch.cat(all_targets, dim=0)
            
            # è®¡ç®—å¸§çº§æŒ‡æ ‡
            frame_metrics = self.metric_tracker.compute_frame_metrics(
                all_predictions, all_targets
            )
            
            # è®¡ç®—ç±»åˆ«çº§æŒ‡æ ‡
            class_metrics = self.metric_tracker.compute_class_wise_metrics(
                all_predictions, all_targets
            )
            
            val_metrics = {
                'frame_accuracy': frame_metrics['frame_accuracy'],
                'class_f1_scores': class_metrics['class_f1_scores'],
                'avg_class_f1': class_metrics['avg_class_f1']
            }
        else:
            val_metrics = {}
        
        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')
        
        return avg_loss, val_metrics
    
    def get_dcase_training_summary(self):
        """è·å–DCASEè®­ç»ƒæ‘˜è¦"""
        summary = {
            'model_info': {
                'num_classes': self.num_classes,
                'class_names': self.class_names,
                'total_params': sum(p.numel() for p in self.model.parameters()),
                'trainable_params': sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            },
            'training_state': {
                'current_epoch': self.current_epoch,
                'best_loss': self.best_loss,
                'learning_rate': self.optimizer.param_groups[0]['lr']
            },
            'metrics': self.metric_tracker.get_summary()
        }
        
        return summary
    
    def save_dcase_checkpoint(self, epoch, save_path, val_metrics=None):
        """ä¿å­˜DCASEæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'scaler_state_dict': self.scaler.state_dict(),
            'best_loss': self.best_loss,
            'best_model_state': self.best_model_state,
            'training_state': self.training_state,
            'val_metrics': val_metrics,
            'dcase_config': {
                'num_classes': self.num_classes,
                'class_names': self.class_names
            },
            'metric_history': self.metric_tracker.get_summary()
        }
        
        torch.save(checkpoint, save_path)
        logging.info(f"DCASEæ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {save_path}")
        
        return checkpoint


def create_dcase_trainer(model, device):
    """åˆ›å»ºDCASEè®­ç»ƒå™¨çš„å·¥å‚å‡½æ•°"""
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = cfg.DCASE_TRAIN_CONFIG['optimizer'](
        model.parameters(),
        **cfg.DCASE_TRAIN_CONFIG['optimizer_params']
    )
    
    # æŸå¤±å‡½æ•°
    loss_fn = cfg.loss_fn
    
    # æ··åˆç²¾åº¦ç¼©æ”¾å™¨
    scaler = amp.GradScaler(enabled=cfg.DCASE_TRAIN_CONFIG['mixed_precision'])
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DCASETrainer(
        model=model,
        optimizer=optimizer,
        loss_fn=loss_fn,
        device=device,
        scaler=scaler
    )
    
    return trainer


if __name__ == "__main__":
    # æµ‹è¯•DCASEè®­ç»ƒå™¨
    print("ğŸ§ª æµ‹è¯•DCASEè®­ç»ƒå™¨...")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„å•å…ƒæµ‹è¯•
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºdummyæ¨¡å‹è¿›è¡Œæµ‹è¯•
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv = nn.Conv2d(1, 32, 3, padding=1)
            self.pool = nn.AdaptiveAvgPool2d((313, 1))
            self.fc = nn.Linear(32, 10)
            
        def forward(self, x):
            # x: [batch, 1, 128, 313]
            x = self.conv(x)  # [batch, 32, 128, 313]
            x = self.pool(x)  # [batch, 32, 313, 1]
            x = x.squeeze(-1).transpose(1, 2)  # [batch, 313, 32]
            x = self.fc(x)  # [batch, 313, 10]
            return x
    
    model = DummyModel().to(device)
    trainer = create_dcase_trainer(model, device)
    
    print(f"âœ… DCASEè®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
    print(f"   ç±»åˆ«æ•°: {trainer.num_classes}")
    print(f"   ç±»åˆ«å: {trainer.class_names}")
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")

