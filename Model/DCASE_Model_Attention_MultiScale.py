#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DCASE2020ä¸“ç”¨çš„å¤šå°ºåº¦æ³¨æ„åŠ›æ¨¡å‹
é€‚é…DCASE2020æ•°æ®é›†çš„10ç±»å£°éŸ³äº‹ä»¶æ£€æµ‹ä»»åŠ¡

åŸºäºåŸå§‹Whale_Model_Attention_MultiScaleï¼Œä½†ä½¿ç”¨DCASEé…ç½®
"""

import torch
from torch import nn
import sys
from pathlib import Path

# æ·»åŠ æ¨¡å‹è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from BA_Conv import BSA_Conv
from Spatial_shift_modified import Spatial_shift
from MultiScale_Ldsa import MultiScaleLDSA
import config_dcase as cfg


class DCASE_Model_Attention_MultiScale(nn.Module):
    """ä½¿ç”¨å¤šå°ºåº¦LDSAçš„DCASEå£°éŸ³äº‹ä»¶æ£€æµ‹æ¨¡å‹
    
    ä¸“é—¨é€‚é…DCASE2020æ•°æ®é›†ï¼š
    - è¾“å…¥ï¼š128ä¸ªMelæ»¤æ³¢å™¨ Ã— 313å¸§
    - è¾“å‡ºï¼š10ç±»å£°éŸ³äº‹ä»¶çš„å¸§çº§é¢„æµ‹
    """

    def __init__(self):
        super().__init__()
        
        # ä½¿ç”¨DCASEé…ç½®
        self.config = cfg.DCASE_MODEL_CONFIG
        self.audio_config = cfg.DCASE_AUDIO_CONFIG

        # éªŒè¯è¾“å…¥ç»´åº¦ - DCASEä½¿ç”¨ACTç‰¹å¾ï¼ˆ512 freq binsï¼‰
        input_freq = self.audio_config['freq']  # 512 (ACTç‰¹å¾)
        if input_freq & (input_freq - 1) != 0:
            # å¦‚æœä¸æ˜¯2çš„å¹‚ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´å·ç§¯å‚æ•°
            print(f"[Warning] Input frequency dimension {input_freq} is not a power of 2")

        # éªŒè¯æ³¨æ„åŠ›å¤´æ•°
        n_feat = self.config['feature_dims']['n_feat']  # 256 (32*8)
        n_head = self.config['attention']['n_head']     # 8
        if n_feat % n_head != 0:
            raise ValueError(f"Feature dimension {n_feat} must be divisible by attention heads {n_head}")

        print(f"[Info] Creating DCASE model:")
        print(f"   Input dimensions: {input_freq} x {self.audio_config['frame']}")
        print(f"   Feature dimensions: {n_feat}")
        print(f"   Attention heads: {n_head}")
        print(f"   Output classes: {self.config['num_classes']}")
        
        # ç‰¹å¾æå–å±‚ - ä½¿ç”¨DCASEé…ç½®
        self.bsa_conv1 = BSA_Conv(**self.config['bsa_conv1'])
        self.spatial_shift = Spatial_shift(
            in_channel=self.config['bsa_conv1']['c3_out']
        )
        self.bsa_conv2 = BSA_Conv(**self.config['bsa_conv2'])
        
        # ä½¿ç”¨å¤šå°ºåº¦LDSA - ä½¿ç”¨DCASEé…ç½®
        self.ldsa = MultiScaleLDSA(
            n_head=self.config['attention']['n_head'],
            n_feat=self.config['feature_dims']['n_feat'],
            dropout_rate=self.config['attention']['dropout'],
            context_sizes=cfg.DCASE_DDSA_CONFIG['context_sizes'],  # [21, 41, 61, 81]
            use_bias=cfg.DCASE_LDSA_CONFIG['use_bias']
        )
        
        # GRUå±‚ - ä½¿ç”¨DCASEé…ç½®
        self.gru = nn.GRU(
            input_size=self.config['attention']['n_feat'],
            hidden_size=self.config['gru']['hidden_size'],
            num_layers=self.config['gru']['num_layers'],
            batch_first=True,
            bidirectional=True,
            dropout=self.config['gru']['dropout'] 
            if self.config['gru']['num_layers'] > 1 else 0
        )
        
        # åˆ†ç±»å±‚ - è¾“å‡ºDCASEçš„10ä¸ªç±»åˆ«
        self.fc = nn.Linear(
            self.config['gru']['hidden_size'] * 2,  # åŒå‘GRU
            self.config['num_classes']  # 10ç±»
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x):
        """å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥ç‰¹å¾ [B, F, T] æˆ– [B, C, F, T]
               - B: batch size
               - C: é€šé“æ•° (é€šå¸¸ä¸º1,å¯é€‰)
               - F: é¢‘ç‡ç»´åº¦ (DCASE: 512 for ACT features)
               - T: æ—¶é—´ç»´åº¦ (DCASE: 311)

        Returns:
            output: [B, T, num_classes] å¸§çº§é¢„æµ‹
        """
        # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if x.dim() == 3:  # [B, F, T]
            x = x.unsqueeze(1)  # æ·»åŠ é€šé“ç»´åº¦ -> [B, 1, F, T]
        elif x.dim() != 4:
            raise ValueError(f"Expected 3D or 4D input, got {x.dim()}D: {x.shape}")

        # è®°å½•è¾“å…¥å½¢çŠ¶ç”¨äºè°ƒè¯•
        input_shape = x.shape

        # ç‰¹å¾æå–
        x = self.bsa_conv1(x)  # [B, 8, F/4, T]
        x = self.spatial_shift(x)  # [B, 8, F/4, T]
        x = self.bsa_conv2(x)  # [B, 32, F/16, T]

        # è·å–ç‰¹å¾ç»´åº¦
        B, C, F, T = x.shape

        # è°ƒæ•´ç»´åº¦ä¸ºåºåˆ—æ ¼å¼
        # [B, C, F, T] -> [B, F, C, T] -> [B, F*C, T] -> [B, T, F*C]
        x = x.transpose(1, 2)  # [B, F, C, T]
        x = x.reshape(B, F * C, T)  # [B, F*C, T] -> [B, n_feat, T]
        x = x.transpose(1, 2)  # [B, T, n_feat]

        # å¤šå°ºåº¦LDSAæ³¨æ„åŠ›
        x = self.ldsa(x, x, x)  # [B, T, F*C] -> [B, 313, 512]

        # GRUå¤„ç†
        gru_out, _ = self.gru(x)  # [B, T, H*2] -> [B, 313, 512]

        # åˆ†ç±» - è¾“å‡ºå¸§çº§é¢„æµ‹
        output = self.fc(gru_out)  # [B, T, num_classes] -> [B, 313, 10]

        return output

    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        return {
            'model_name': 'DCASE_Model_Attention_MultiScale',
            'total_params': total_params,
            'trainable_params': trainable_params,
            'input_shape': (1, self.audio_config['freq'], self.audio_config['frame']),
            'output_shape': (self.audio_config['frame'], self.config['num_classes']),
            'num_classes': self.config['num_classes'],
            'class_names': self.config['class_names']
        }


def create_dcase_model():
    """åˆ›å»ºDCASEæ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    return DCASE_Model_Attention_MultiScale()


# æµ‹è¯•å‡½æ•°
def test_dcase_model():
    """æµ‹è¯•DCASEæ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•DCASEæ¨¡å‹...")
    
    model = create_dcase_model()
    model_info = model.get_model_info()
    
    print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
    for key, value in model_info.items():
        print(f"   {key}: {value}")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    freq_bins = cfg.DCASE_AUDIO_CONFIG['freq']  # ä½¿ç”¨ACTç‰¹å¾ç»´åº¦ï¼ˆ512ï¼‰
    frames = cfg.DCASE_AUDIO_CONFIG['frame']

    test_input = torch.randn(batch_size, 1, freq_bins, frames)
    print(f"\nğŸ“Š æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    model.eval()
    with torch.no_grad():
        try:
            output = model(test_input)
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"ğŸ“¤ è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"   é¢„æœŸ: [batch_size, frames, num_classes] = [{batch_size}, {frames}, {cfg.DCASE_MODEL_CONFIG['num_classes']}]")
            
            # æ£€æŸ¥è¾“å‡ºèŒƒå›´
            print(f"ğŸ“ˆ è¾“å‡ºç»Ÿè®¡:")
            print(f"   æœ€å°å€¼: {output.min().item():.4f}")
            print(f"   æœ€å¤§å€¼: {output.max().item():.4f}")
            print(f"   å‡å€¼: {output.mean().item():.4f}")
            print(f"   æ ‡å‡†å·®: {output.std().item():.4f}")
            
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_dcase_model()
